{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xn4PKWA1qrat",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xn4PKWA1qrat",
        "outputId": "7723b9f1-4a1f-4661-ee87-c895f8ed790a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Mount google drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Install gym\n",
        "# !pip install gym==0.25\n",
        "\n",
        "# # Downgrade to lower numpy\n",
        "# !pip install numpy==1.23.5\n",
        "\n",
        "# Make the directory\n",
        "!mkdir -p video"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "X-9hhj_QkntM",
      "metadata": {
        "id": "X-9hhj_QkntM"
      },
      "source": [
        "# REINFORCE on CartPole-v0\n",
        "\n",
        "Algorithm is reported in slide 53:\n",
        "\n",
        "**REINFORCE** is acronym for \"**RE**ward **I**ncrement = **N**onnegative **F**actor * **O**ffset **R**einforcement * **C**haracteristic **E**ligibility\".\n",
        "\n",
        "The algorithm ([Williams, 1992](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf)) is a monte carlo variation of policy gradient algorithm in RL. The agent collects the trajectory of an episode from current policy. Usually, this policy depends on the policy parameter which denoted as $\\theta$.\n",
        "\n",
        "The tools that are used for this demo are:\n",
        "\n",
        "1. Google Collab\n",
        "2. Pytorch\n",
        "3. OpenAI gym CartPole-v0\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TuJff8gskntQ",
      "metadata": {
        "id": "TuJff8gskntQ"
      },
      "outputs": [],
      "source": [
        "# Import required methods\n",
        "\n",
        "import gym  # GPU accelerated RL framework from OpenAI, NVIDIA, ETHZ ...\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = (16, 10)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "torch.manual_seed(0)\n",
        "\n",
        "import base64, io, os\n",
        "\n",
        "# For visualization\n",
        "from gym.wrappers import RecordVideo\n",
        "from IPython.display import HTML\n",
        "from IPython import display\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc36WA3gkntS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc36WA3gkntS",
        "outputId": "79efd6d7-c86f-4e84-a822-6d3b75b7098c"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vxQ2wOjckntT",
      "metadata": {
        "id": "vxQ2wOjckntT"
      },
      "source": [
        "### Instantiate the Environment and Agent\n",
        "\n",
        "CartPole environment is very simple. It has discrete action space (2) and 4 dimensional state space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d-19DX43kntT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-19DX43kntT",
        "outputId": "7d6b321d-3b76-4275-9510-a476ed6e1c72"
      },
      "outputs": [],
      "source": [
        "env = gym.make('CartPole-v1') # Reference: https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
        "env.seed(0)\n",
        "\n",
        "# Change the default maximum steps\n",
        "env._max_episode_steps = 1000\n",
        "\n",
        "print('observation space:', env.observation_space)\n",
        "print('action space:', env.action_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Idcky0whkntT",
      "metadata": {
        "id": "Idcky0whkntT"
      },
      "source": [
        "### Define Policy\n",
        "Unlike value-based method, the output of policy-based method is the probability of each action. It can be represented as policy. So activation function of output layer will be softmax, not ReLU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HgvvqAPSkntU",
      "metadata": {
        "id": "HgvvqAPSkntU"
      },
      "outputs": [],
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, state_size=4, action_size=2, hidden_size=32):\n",
        "        super(Policy, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = self.fc2(x)\n",
        "        # we just consider 1 dimensional probability of action\n",
        "        return F.softmax(x, dim=1)\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        probs = self.forward(state).cpu()\n",
        "        model = Categorical(probs)\n",
        "        action = model.sample()\n",
        "        return action.item(), model.log_prob(action)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0kET5hUFkntU",
      "metadata": {
        "id": "0kET5hUFkntU"
      },
      "source": [
        "### REINFORCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WknuLvM6kntU",
      "metadata": {
        "id": "WknuLvM6kntU"
      },
      "outputs": [],
      "source": [
        "def reinforce(policy, optimizer, n_episodes=1000, max_t=1000, gamma=0.99, print_every=100, pass_score=200):\n",
        "\n",
        "    scores_window = deque(maxlen=20)\n",
        "    scores_by_eps = []\n",
        "    scores_smooth = []\n",
        "\n",
        "    for e in range(1, n_episodes):\n",
        "\n",
        "        # Initialize logs\n",
        "        lnpis_buf   = []\n",
        "        rewards_buf = []\n",
        "\n",
        "        # Get state at time 0\n",
        "        state = env.reset()\n",
        "\n",
        "        # Collect trajectory\n",
        "        for t in range(max_t):\n",
        "\n",
        "            # Sample the action from current policy\n",
        "            action, lnpi = policy.act(state)\n",
        "            lnpis_buf.append(lnpi)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            rewards_buf.append(reward)\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Calculate total reward\n",
        "        scores_window.append(sum(rewards_buf))\n",
        "        scores_by_eps.append(sum(rewards_buf))\n",
        "        scores_smooth.append(np.mean(scores_window))\n",
        "\n",
        "        # Calculate the return by applying discounted factor to the rewards\n",
        "        t_steps            = np.arange(len(rewards_buf))\n",
        "        discounted_rewards = [ (gamma**i) * r for i, r in enumerate(rewards_buf) ]\n",
        "        returns_buf        = np.cumsum(discounted_rewards[::-1])[::-1] / gamma ** t_steps\n",
        "\n",
        "        # Calculate the loss\n",
        "        policy_loss = []\n",
        "        for lnpi, gt in zip(lnpis_buf, returns_buf):\n",
        "            # Note that we are using Gradient Ascent, not Descent. So we need to calculate it with negative rewards.\n",
        "            policy_loss.append(-lnpi * gt)\n",
        "\n",
        "        # After that, we concatenate whole policy loss in 0th dimension\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if e % print_every == 0:\n",
        "            print(f'Episode {e:5d}. Average Score: {np.mean(scores_window):.2f}')\n",
        "\n",
        "        if all(num > pass_score for num in scores_window) :\n",
        "            print(f'Environment solved in {e - 100:d} episodes!\\tAverage Score: {np.mean(scores_window):.2f}')\n",
        "            break\n",
        "\n",
        "    return scores_by_eps, scores_smooth"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-Hl-uQxPkntU",
      "metadata": {
        "id": "-Hl-uQxPkntU"
      },
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VUIVCr1akntV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUIVCr1akntV",
        "outputId": "01b78fa3-b5fe-42d5-deac-96b972828209"
      },
      "outputs": [],
      "source": [
        "pass_score = 400\n",
        "\n",
        "policy = Policy().to(device)\n",
        "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
        "scores, scores_smooth = reinforce(policy, optimizer, max_t=1000, n_episodes=2000, pass_score=pass_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B5nrhJNTkntV",
      "metadata": {
        "id": "B5nrhJNTkntV"
      },
      "source": [
        "### Plot the learning progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vcm7OMenkntV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        },
        "id": "vcm7OMenkntV",
        "outputId": "fedc0f97-37f1-42dd-a7e7-41e71f407136"
      },
      "outputs": [],
      "source": [
        "# plot the scores\n",
        "plt.rcParams.update({\"font.size\": 18})  # Applies to all text elements\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(scores, linewidth=1, color='b')\n",
        "plt.plot(scores_smooth, linewidth=2, color='r')\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Episode #')\n",
        "plt.grid('on')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EDRnojaHkntV",
      "metadata": {
        "id": "EDRnojaHkntV"
      },
      "source": [
        "### Animate it with Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "inSs63hYkntV",
      "metadata": {
        "id": "inSs63hYkntV"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "max_play_t = 1000\n",
        "\n",
        "def show_video(env_name):\n",
        "    mp4list = glob.glob('video/**/*.mp4', recursive=True)\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "def play_model(policy, env_name, max_play_t):\n",
        "    env = gym.make(env_name)\n",
        "    env._max_episode_steps = max_play_t\n",
        "    env = RecordVideo(env, f\"video/{env_name}\", new_step_api=True)\n",
        "    state = env.reset()\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "    for t in range(max_play_t):\n",
        "        # vid.capture_frame()\n",
        "        action, _ = policy.act(state)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        state = next_state\n",
        "        if terminated or truncated:\n",
        "            print(\"Done by termination or truncation:\", terminated, truncated, t)\n",
        "            break\n",
        "    if t >= max_play_t-1:\n",
        "        print(\"Done by termination or truncation\", terminated, truncated, t)\n",
        "    # vid.close()\n",
        "    env.close()\n",
        "    \n",
        "play_model(policy, 'CartPole-v1', max_play_t)\n",
        "show_video('CartPole-v1')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
